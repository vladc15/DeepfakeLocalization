{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c92a6c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c92a6c3e",
        "outputId": "6bf72254-3d8a-48a4-c0dc-7a9bfbb0a7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "odcHb42V9rww",
      "metadata": {
        "id": "odcHb42V9rww"
      },
      "outputs": [],
      "source": [
        "# update working directory\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-custom')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Za2wYqKP9u3G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Za2wYqKP9u3G",
        "outputId": "c52a6fdc-d99f-409a-c681-c77cbd32d6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xy3QQDZq9sCc",
      "metadata": {
        "id": "xy3QQDZq9sCc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch.multiprocessing\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score, accuracy_score\n",
        "\n",
        "from parameters import Parameters\n",
        "from train.trainer import Trainer\n",
        "from deepfake_datasets.datasets import get_dataloader\n",
        "from train.early_stopping import EarlyStopping\n",
        "from train.validate import validate_detection, validate_fully_supervised_localization, validate_weakly_supervised_localization\n",
        "from utils.utils import compute_mean_iou, compute_mean_ap, compute_mean_f1, compute_mean_acc_detection, compute_mean_ap_detection, compute_batch_iou, compute_batch_localization_f1, compute_batch_ap, compute_accuracy_detection, compute_average_precision_detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4118cc",
      "metadata": {
        "id": "6c4118cc"
      },
      "outputs": [],
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VFIwhSdfp2t1",
      "metadata": {
        "id": "VFIwhSdfp2t1"
      },
      "outputs": [],
      "source": [
        "new_dataset_name = 'pluralistic'\n",
        "\n",
        "zip_path = f'/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-main/datasets_zip/{new_dataset_name}.zip'\n",
        "new_content_root_path = f'/content/datasets/dolos_data/celebahq/fake/'\n",
        "\n",
        "os.makedirs(new_content_root_path, exist_ok=True)\n",
        "\n",
        "# move the dataset from drive to /content (SSD) for better performance in I/O\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(new_content_root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09390516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09390516",
        "outputId": "de21e146-3a92-4b3f-d891-e96b6ead9b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experiment_name: training_vit_weakly_supervised\n",
            "task_type: weakly_supervised_localization\n",
            "data_label: train\n",
            "arch: CLIP:ViT-L/14\n",
            "fix_backbone: True\n",
            "weight_decay: 0.0\n",
            "batch_size: 64\n",
            "num_threads: 8\n",
            "init_type: normal\n",
            "init_gain: 0.02\n",
            "train_dataset: pluralistic\n",
            "decoder_type: conv-20\n",
            "feature_layer: layer20\n",
            "early_stop_epochs: 5\n",
            "optim: adam\n",
            "beta1: 0.9\n",
            "lr: 0.001\n",
            "show_loss_freq: 50\n",
            "num_iter: 30\n",
            "data_root_path: /content/datasets/dolos_data/celebahq\n",
            "train_fake_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/images/train\n",
            "valid_fake_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/images/valid\n",
            "test_fake_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/images/test\n",
            "train_masks_ground_truth_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/masks/train\n",
            "valid_masks_ground_truth_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/masks/valid\n",
            "test_masks_ground_truth_path: /content/datasets/dolos_data/celebahq/fake/pluralistic/masks/test\n",
            "train_real_path: /content/datasets/dolos_data/celebahq/real/train\n",
            "valid_real_path: /content/datasets/dolos_data/celebahq/real/valid\n",
            "test_real_path: /content/datasets/dolos_data/celebahq/real/test\n",
            "checkpoint_path: \n",
            "save_dir: experiments/training_vit_weakly_supervised\n",
            "save_dir_models: experiments/training_vit_weakly_supervised/models\n",
            "save_dir_results: experiments/training_vit_weakly_supervised/results\n",
            "loss_type: bce\n",
            "weakly_supervised_label_comparison_type: expansion\n"
          ]
        }
      ],
      "source": [
        "# set parameters\n",
        "params = Parameters()\n",
        "\n",
        "# set experiment name\n",
        "params.experiment_name = 'training_vit_weakly_supervised'\n",
        "\n",
        "# set backbone\n",
        "params.arch = 'CLIP:ViT-L/14'\n",
        "\n",
        "# create output dirs\n",
        "params.create_output_dirs()\n",
        "\n",
        "# uncomment lines below to set new datasets\n",
        "# new_drive_root_path = '/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-main/datasets/dolos_data/celebahq/'\n",
        "# new_root_path = '/content/dataset'\n",
        "# new_root_path = '..\\\\DeCLIP-main\\\\datasets\\\\dolos_data\\\\celebahq\\\\'\n",
        "# new_root_path = 'D:\\\\Python\\\\DeCLIP-main\\\\datasets\\\\dolos_data\\\\celebahq\\\\'\n",
        "\n",
        "\n",
        "# move the dataset from drive to /content for better performance in I/O\n",
        "# shutil.copytree(new_drive_root_path, new_root_path, dirs_exist_ok=True)\n",
        "\n",
        "new_root_path = '/content/datasets/dolos_data/celebahq/'\n",
        "new_dataset_name = 'pluralistic'\n",
        "\n",
        "params.update_dolos_data_paths(new_root_path, new_dataset_name)\n",
        "\n",
        "\n",
        "# fix the backbone - to train only the decoder\n",
        "params.fix_backbone = True\n",
        "\n",
        "# set the feature layer and the decoder type\n",
        "params.feature_layer = 'layer20'\n",
        "params.decoder_type = 'conv-20'\n",
        "\n",
        "# set the batch size and num threads\n",
        "params.batch_size = 64\n",
        "params.num_threads = 8\n",
        "\n",
        "# set the number of epochs\n",
        "params.num_iter = 30\n",
        "\n",
        "# set loss type\n",
        "params.loss_type = 'bce'\n",
        "\n",
        "# task\n",
        "params.task_type = 'weakly_supervised_localization'\n",
        "params.weakly_supervised_label_comparison_type = 'expansion'\n",
        "\n",
        "for key, value in params.__dict__.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d032661",
      "metadata": {
        "id": "1d032661"
      },
      "outputs": [],
      "source": [
        "# get the model, along with its trainer\n",
        "model_trainer = Trainer(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c6fd79",
      "metadata": {
        "id": "e3c6fd79"
      },
      "outputs": [],
      "source": [
        "# get the dataloaders\n",
        "train_loader = get_dataloader(params)\n",
        "\n",
        "val_params = deepcopy(params)\n",
        "val_params.data_label = 'valid'\n",
        "val_loader = get_dataloader(val_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "MMgwNMsDacBg"
      },
      "id": "MMgwNMsDacBg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afffa2da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "afffa2da",
        "outputId": "60f432b2-6983-4683-f515-8c22622383f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training set: 9000\n",
            "Length of validation set: 900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 0.3174 at step 50 \t Iter time: 2.58\n",
            "Train Loss: 0.3138 at step 100 \t Iter time: 2.56\n",
            "Average Train Loss: 0.0001\n",
            "Validation:\n",
            "Length of dataset:  900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:03<00:48,  3.43s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:05<00:32,  2.53s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:07<00:26,  2.25s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:09<00:23,  2.11s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:11<00:20,  2.04s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:17,  1.99s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:15,  1.96s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:13,  1.94s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:11,  1.93s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:09,  1.92s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:07,  1.91s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:05,  1.91s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:03,  1.91s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:01,  1.90s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:28<00:00,  1.89s/it]\n",
            "  3%|▎         | 1/30 [06:25<3:06:23, 385.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "Train Loss: 0.3135 at step 150 \t Iter time: 2.73\n",
            "Train Loss: 0.3134 at step 200 \t Iter time: 2.68\n",
            "Train Loss: 0.3134 at step 250 \t Iter time: 2.65\n",
            "Average Train Loss: 0.0001\n",
            "Validation:\n",
            "Length of dataset:  900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:03<00:48,  3.45s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:05<00:33,  2.54s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:07<00:27,  2.25s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:09<00:23,  2.11s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:11<00:20,  2.04s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:17,  1.99s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:15,  1.96s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:13,  1.94s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:11,  1.93s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:09,  1.92s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:07,  1.91s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:05,  1.91s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:03,  1.91s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:01,  1.90s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:28<00:00,  1.89s/it]\n",
            "  7%|▋         | 2/30 [12:49<2:59:30, 384.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2\n",
            "Train Loss: 0.3133 at step 300 \t Iter time: 2.72\n",
            "Train Loss: 0.3133 at step 350 \t Iter time: 2.69\n",
            "Train Loss: 0.3133 at step 400 \t Iter time: 2.67\n",
            "Average Train Loss: 0.0001\n",
            "Validation:\n",
            "Length of dataset:  900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:03<00:48,  3.45s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:05<00:33,  2.54s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:07<00:27,  2.25s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:09<00:23,  2.11s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:11<00:20,  2.04s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:17,  1.99s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:15,  1.96s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:13,  1.94s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:11,  1.93s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:09,  1.92s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:07,  1.91s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:05,  1.91s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:03,  1.91s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:01,  1.90s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:28<00:00,  1.89s/it]\n",
            " 10%|█         | 3/30 [19:13<2:52:59, 384.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [19:44<2:57:42, 394.91s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cd79733baf5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-custom/train/trainer.py\u001b[0m in \u001b[0;36mset_input\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# start the training loop\n",
        "torch.cuda.empty_cache()\n",
        "# model_trainer.compute_pos_weight_per_dataset(train_loader)\n",
        "early_stopping = EarlyStopping(patience=params.early_stop_epochs, verbose=True, delta=-0.001)\n",
        "best_metric = 0.0 # iou for localization, ap for detection\n",
        "print('Length of training set:', len(train_loader.dataset))\n",
        "print('Length of validation set:', len(val_loader.dataset))\n",
        "start_time = time.time()\n",
        "for epoch in tqdm(range(params.num_iter)):\n",
        "    print('Epoch:', epoch)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for data in train_loader:\n",
        "        model_trainer.total_steps += 1\n",
        "\n",
        "        model_trainer.set_input(data)\n",
        "        model_trainer.optimize_parameters()\n",
        "\n",
        "        if model_trainer.total_steps % params.show_loss_freq == 0:\n",
        "            epoch_loss += model_trainer.loss.item()\n",
        "            print(f'Train Loss: {model_trainer.loss.item():.4f} at step {model_trainer.total_steps} \\t Iter time: {(time.time() - start_time) / model_trainer.total_steps:.2f}')\n",
        "\n",
        "\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "    print(f'Average Train Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # compute training metrics\n",
        "    if params.task_type == 'fully_supervised_localization':\n",
        "        compute_mean_iou(model_trainer.ious, verbose=True, extra_text=f'Train epoch {epoch} ')\n",
        "        model_trainer.ious = []\n",
        "\n",
        "        compute_mean_f1(model_trainer.f1_best, model_trainer.f1_fixed, verbose=True, extra_text=f'Train epoch {epoch} ')\n",
        "        model_trainer.f1_best = []\n",
        "        model_trainer.f1_fixed = []\n",
        "\n",
        "        compute_mean_ap(model_trainer.ap, verbose=True, extra_text=f'Train epoch {epoch} ')\n",
        "        model_trainer.ap = []\n",
        "\n",
        "    elif params.task_type == 'detection':\n",
        "        model_trainer.format_output_detection()\n",
        "\n",
        "        # compute_mean_acc_detection(model_trainer.logits, model_trainer.labels, verbose=True, extra_text=f'Train epoch {epoch} ')\n",
        "        # compute_mean_ap_detection(model_trainer.logits, model_trainer.labels, verbose=True, extra_text=f'Train epoch {epoch} ')\n",
        "\n",
        "        logits_np = model_trainer.logits.detach().cpu().numpy()\n",
        "        labels_np = model_trainer.labels.detach().cpu().numpy()\n",
        "\n",
        "        num_classes = logits_np.shape[1]\n",
        "        labels_np = labels_np.astype(int)\n",
        "        labels_one_hot = np.eye(num_classes)[labels_np]\n",
        "\n",
        "        ap = average_precision_score(labels_one_hot, logits_np, average='macro')\n",
        "        acc = accuracy_score(labels_np, np.argmax(logits_np, axis=1))\n",
        "\n",
        "        print(f'Train epoch {epoch} Mean ACC: {ap:.4f}')\n",
        "        print(f'Train epoch {epoch} Mean AP: {acc:.4f}')\n",
        "\n",
        "        model_trainer.logits = []\n",
        "        model_trainer.labels = []\n",
        "\n",
        "\n",
        "    # validate the model\n",
        "    print('Validation:')\n",
        "    if params.task_type == 'fully_supervised_localization':\n",
        "        ious, f1_best, f1_fixed, mean_ap, _ = validate_fully_supervised_localization(model_trainer.model, val_loader, params.train_dataset)\n",
        "\n",
        "        # compute metrics\n",
        "        mean_iou = compute_mean_iou(ious, verbose=True, extra_text=f'Validation at epoch {epoch} ')\n",
        "\n",
        "        mean_f1_best, mean_f1_fixed = compute_mean_f1(f1_best, f1_fixed, verbose=True, extra_text=f'Validation at epoch {epoch} ')\n",
        "\n",
        "        mean_ap = compute_mean_ap(mean_ap, verbose=True, extra_text=f'Validation at epoch {epoch} ')\n",
        "\n",
        "        # save the model if the mean iou is improved\n",
        "        if mean_iou > best_metric:\n",
        "            best_metric = mean_iou\n",
        "            model_trainer.save_model(f'best_localization_model_iou_{mean_iou:.4f}.pth')\n",
        "            print(f'Best model saved at epoch {epoch}!')\n",
        "\n",
        "        # check for early stopping\n",
        "        early_stopping(mean_iou)\n",
        "\n",
        "    elif params.task_type == 'detection':\n",
        "        ap, acc, _ = validate_detection(model_trainer.model, val_loader)\n",
        "\n",
        "        print(f'Validation at epoch {epoch} - AP: {ap:.4f}, Acc: {acc:.4f}')\n",
        "\n",
        "        # save the model if the mean ap is improved\n",
        "        if ap > best_metric:\n",
        "            best_metric = ap\n",
        "            model_trainer.save_model(f'best_detection_model_ap_{ap:.4f}.pth')\n",
        "            print(f'Best model saved at epoch {epoch}!')\n",
        "\n",
        "        # check for early stopping\n",
        "        early_stopping(ap)\n",
        "\n",
        "    elif params.task_type == 'weakly_supervised_localization':\n",
        "        ap, acc, _ = validate_weakly_supervised_localization(model_trainer.model, val_loader, params.weakly_supervised_label_comparison_type, params.train_dataset)\n",
        "\n",
        "        if ap is not None:\n",
        "            print(f'Validation at epoch {epoch} - AP: {ap:.4f}, Acc: {acc:.4f}')\n",
        "\n",
        "        # save the model if the mean ap is improved\n",
        "        if ap is not None and ap > best_metric:\n",
        "            best_metric = ap\n",
        "            model_trainer.save_model(f'best_detection_model_ap_{ap:.4f}.pth')\n",
        "            print(f'Best model saved at epoch {epoch}!')\n",
        "\n",
        "        # check for early stopping\n",
        "        early_stopping(ap)\n",
        "\n",
        "    # check if early stopping is triggered\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        continue_training = model_trainer.adjust_learning_rate()\n",
        "        if continue_training:\n",
        "            print(\"Continuing training with a learning rate reduced by a factor of 10\")\n",
        "            early_stopping = EarlyStopping(patience=params.early_stop_epochs, verbose=True, delta=-0.002) # adjust the delta only once, otherwise stop completely\n",
        "        else:\n",
        "            print(f\"Early stopping training at epoch {epoch}\")\n",
        "            break\n",
        "    print()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "piCscRRUn-C8",
      "metadata": {
        "id": "piCscRRUn-C8"
      },
      "outputs": [],
      "source": [
        "# model_dir = '/content/experiments/training_vit_weakly_supervised/models/'\n",
        "# model_files = sorted(\n",
        "#     [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "# )\n",
        "\n",
        "# best_model = model_files[-1]\n",
        "# best_model_path = os.path.join(model_dir, best_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil.copy(best_model_path, f'/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-custom/trained_models/weakly_supervised/{os.path.basename(best_model_path)}')"
      ],
      "metadata": {
        "id": "odNzGRUrwHI9"
      },
      "id": "odNzGRUrwHI9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer.save_model(f'best_weakly_supervised_model_epoch_3_expansion_pluralistic.pth')"
      ],
      "metadata": {
        "id": "XU5a8mmrgHQq"
      },
      "id": "XU5a8mmrgHQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_model_path = '/content/experiments/training_vit_weakly_supervised/models/best_weakly_supervised_model_epoch_3_expansion_pluralistic.pth'\n",
        "shutil.copy(last_model_path, f'/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-custom/trained_models/weakly_supervised/{os.path.basename(last_model_path)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wHlN1LYpgHhP",
        "outputId": "4cc79807-97aa-44c8-d1af-e7d374c3348a"
      },
      "id": "wHlN1LYpgHhP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Licenta/DeCLIP-custom/trained_models/weakly_supervised/best_weakly_supervised_model_epoch_3_expansion_pluralistic.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}